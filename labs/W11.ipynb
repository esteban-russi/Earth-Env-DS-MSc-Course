{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W11: High-Performance Computing and Dask for Parallel Computing\n",
    "- Contributer: Dr. Zhonghua Zheng, Yuan Sun\n",
    "- Course Unit: Earth and Environmental Data Science (EART60702)\n",
    "- Last modified date: 19 April, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intended Learning Outcomes (ILOs)\n",
    "- High-Performance Computing: Gain a practical understanding of accessing HPC systems and running applications on HPC systems.\n",
    "\n",
    "- Dask for Parallel Computing: Learn to use Dask for parallel data processing and computing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. High-Performance Computing System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HPC systems are designed to provide significantly higher computational power. HPC clusters typically consist of multiple nodes, each containing powerful processors (CPUs) and sometimes specialized accelerators such as GPUs.\n",
    "- HPC systems are optimized for parallel computing, where multiple processing units work together to run jos simultaneously.\n",
    "- HPC at The University of Manchester: https://ri.itservices.manchester.ac.uk/csf3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "log in through SSH remote, the operation system is Linux, Batch system is PBS: https://openpbs.org/\n",
    "- in windows, use the built-in OpenSSH\n",
    "- in linux, use 'ssh' directly through terminal\n",
    "\n",
    "\n",
    "```bash\n",
    "ssh UserName@10.141.12.196\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Run the commands below:\n",
    "```bash\n",
    "touch my_script.sh # create a script file to run jobs\n",
    "vim my_script.sh # write in scripts\n",
    "```\n",
    "\n",
    "The script should be:\n",
    "```\n",
    "#!/bin/bash\n",
    "export OMP_NUM_THREADS=4\n",
    "# use 1 node with 4 processors \n",
    "# This is a simple Bash script\n",
    "echo \"Hello, world!\"\n",
    "```\n",
    "\n",
    "Run the commands below:\n",
    "```bash\n",
    "chmod +x my_script.sh\n",
    "qsub my_script.sh # submit job to the batch system (pbs)\n",
    "qstat # check state\n",
    "cat my_script.sh.o3 # view output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: try to run a job with**\n",
    "- creat a folder 'hpc'\n",
    "- creat a script file 'loop.sh'\n",
    "- use loop.sh to create five scripts files('1.sh', '2.sh', ...'5'.sh ) under the ./hpc/job_script directory, and write in 'Hello, world!' for each file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "Run the commands below:\n",
    "```bash\n",
    "mkdir -p hpc/job_script\n",
    "cd hpc\n",
    "touch loop.sh\n",
    "vim loop.sh\n",
    "```\n",
    "\n",
    "The script should be:\n",
    "```\n",
    "#!/bin/bash\n",
    "NUM_SCRIPT=5\n",
    "for ((i=1; i<=NUM_SCRIPT; i++)); do\n",
    "    SCRIPT=\"./job_script/${NUM_SCRIPT[i]}.sh\"\n",
    "    touch ${SCRIPT}\n",
    "    echo \"Hello, world!\" >> ${SCRIPT}\n",
    "    chmod +x ${SCRIPT}\n",
    "done    \n",
    "```\n",
    "\n",
    "Run the commands below:\n",
    "```bash\n",
    "chmod +x loop.sh\n",
    "./loop.sh # run the script directly\n",
    "qusb loop.sh # run the script through the batch system\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learn about VS Code to faciliate file explorer remotely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dask (20mins)\n",
    "- Dask | Scale the Python tools you love: https://docs.dask.org/en/stable/.\n",
    "- Dask has become a popular choice for data scientists, researchers, and engineers tackling big data and parallel computing challenges in various domains, including scientific computing, machine learning, and data analysis.\n",
    "- Dask provides high-level parallelism and distributed collections that mimic familiar data structures like NumPy array and Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask installation**: https://docs.dask.org/en/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Comparisons in computation speed and result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1: Dask vs NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# traditional way using Numpy\n",
    "x = np.random.random((10000, 10000))\n",
    "y = x * 2\n",
    "\n",
    "mean_y = y.mean(axis=0)\n",
    "sum_y = mean_y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# if using Dask\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000)) \n",
    "# In Dask, chunks is a parameter used to specify how the data should be partitioned or divided into smaller blocks for parallel computation.\n",
    "\n",
    "y = x * 2\n",
    "\n",
    "mean_y = y.mean(axis=0)\n",
    "sum_y = mean_y.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2: Dask vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'feature1': np.random.rand(10000) * 100,\n",
    "    'feature2': np.random.rand(10000) * 100,\n",
    "    'target': np.random.randint(0, 2, 10000)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "ddf = dd.from_pandas(df, npartitions=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['feature3'] = ddf['feature1'] / (ddf['feature2'] + 1)\n",
    "\n",
    "# metic\n",
    "mean_feature1 = ddf['feature1'].mean()\n",
    "mean_feature2 = ddf['feature2'].mean()\n",
    "correlation = ddf[['feature1', 'feature2']].corr().compute()\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Please use Pandas only to repeat the function above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
